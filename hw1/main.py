import re, os, json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def read_csv(file_path):
    """Reads a CSV file and returns a pandas DataFrame."""
    return pd.read_csv(file_path)

def data_cleaning(data_df, value_columns, invalid_pattern):
    # å°æ¯å€‹æ¬„ä½é€²è¡Œæª¢æŸ¥èˆ‡è½‰æ›
    for col in value_columns:
        # æŠŠåŒ…å« invalid çš„åœ°æ–¹è¨­ç‚º NaN
        data_df.iloc[:, col] = data_df.iloc[:, col].astype(str)  # ç¢ºä¿æ˜¯å­—ä¸²
        data_df.iloc[:, col] = data_df.iloc[:, col].mask(
            data_df.iloc[:, col].str.contains(invalid_pattern, case=False, na=False),
            np.nan
        )
        
        # å†æŠŠç„¡æ³•è½‰æˆ float çš„å€¼è¨­ç‚º NaN
        data_df.iloc[:, col] = pd.to_numeric(data_df.iloc[:, col], errors='coerce')
    
    return data_df

# è³‡æ–™å…§æ’
def data_interpolation(data_df, value_columns):
    for i, row in data_df.iterrows():
        # æ‰¾å‡ºè©² row ä¸­ä¸æ˜¯ NaN çš„ value_columns çš„ index
        not_nan = row.iloc[value_columns].notna().to_numpy().nonzero()[0]
        # å¦‚æœæƒ³ä¸åš interpolation çš„è©±ï¼Œéœ€è¦æŠŠé€™äº› row è·³é
        if len(not_nan) == len(value_columns) or len(not_nan) == 0:
            continue
        
        # å¦‚æœè©² row ä¸­æœ‰å¤šå€‹ value_columns ä¸æ˜¯ NaNï¼Œå°±ç”¨ç·šæ€§æ’å€¼æ³• (linear interpolation) è£œå€¼
        np_row = row.iloc[value_columns].to_numpy(dtype=float)
        x = np.arange(len(np_row))
        data_df.iloc[i, value_columns] = np.interp(x, x[not_nan], np_row[not_nan])
    
    return data_df

def data_dropdown(data_df, value_columns, drop_type='item', skip_for_test=False):
    """
    åªä¿ç•™ PM2.5 ç›¸é—œçš„æª¢æŸ¥ï¼Œä¸åˆªé™¤å…¶ä»– feature çš„æ•¸æ“š
    skip_for_test=True æ™‚ï¼Œå®Œå…¨ä¸åˆªé™¤ä»»ä½•æ•¸æ“šï¼ˆç”¨æ–¼æ¸¬è©¦é›†ï¼‰
    """
    if skip_for_test:
        return data_df
    
    dropdown_items = []
    for i, row in data_df.iterrows():
        not_nan = row.iloc[value_columns].notna().to_numpy().nonzero()[0]

        # åªå° PM2.5 (target) åšåš´æ ¼æª¢æŸ¥
        if row.ItemName.strip() == 'PM2.5' and len(not_nan) < 2:
            dropdown_items.append({'date': row.Date, 'feature': row.ItemName})

    for item in dropdown_items:
        if drop_type == 'item':
            data_df = data_df[~((data_df.Date == item['date']) & (data_df.ItemName == item['feature']))]
        elif drop_type == 'date':
            data_df = data_df[data_df.Date != item['date']]
        elif drop_type == 'feature':
            data_df = data_df[data_df.ItemName != item['feature']]
        
    return data_df


# ========== Linear Regression with NaN handling ==========

def train_linear_regression(X, y, w=None, b=None, lr=0.01, iterations=1000, verbose=True):
    """
    ä½¿ç”¨ Gradient Descent è¨“ç·´ Linear Regression æ¨¡å‹
    æ”¯æ´ NaN å€¼è™•ç†ï¼šåœ¨è¨ˆç®—æ™‚å‹•æ…‹å¿½ç•¥ NaN çš„ feature
    """
    n_samples, n_features = X.shape

    # åˆå§‹åŒ–æ¬Šé‡
    if w is None:
        w = np.zeros(n_features)
    if b is None:
        b = 0.0

    for iteration in range(iterations):
        # å°æ¯å€‹æ¨£æœ¬åˆ†åˆ¥è¨ˆç®—é æ¸¬å€¼ï¼ˆå› ç‚ºæ¯å€‹æ¨£æœ¬çš„æœ‰æ•ˆ feature å¯èƒ½ä¸åŒï¼‰
        y_pred = np.zeros(n_samples)
        
        for i in range(n_samples):
            valid_mask = ~np.isnan(X[i])
            if valid_mask.any():
                y_pred[i] = np.dot(X[i][valid_mask], w[valid_mask]) + b
            else:
                y_pred[i] = b  # å¦‚æœæ‰€æœ‰ feature éƒ½æ˜¯ NaNï¼Œåªç”¨ bias
        
        error = y_pred - y
        
        # è¨ˆç®—æ¢¯åº¦ï¼ˆå°æ¯å€‹ feature åˆ†åˆ¥è¨ˆç®—ï¼Œåªç”¨æœ‰æ•ˆçš„æ¨£æœ¬ï¼‰
        dw = np.zeros(n_features)
        for j in range(n_features):
            valid_samples = ~np.isnan(X[:, j])
            if valid_samples.any():
                dw[j] = np.mean(error[valid_samples] * X[valid_samples, j])
        
        db = np.mean(error)
        
        w -= lr * dw
        b -= lr * db

        if verbose and iteration % 10000 == 0:
            rmse = np.sqrt(np.mean(error ** 2))
            print(f"Iteration {iteration:4d} | RMSE = {rmse:.4f}")

    return w, b


def predict(X, w, b):
    """é æ¸¬å‡½æ•¸ï¼Œæ”¯æ´ NaN å€¼"""
    n_samples = X.shape[0]
    predictions = np.zeros(n_samples)
    
    for i in range(n_samples):
        valid_mask = ~np.isnan(X[i])
        if valid_mask.any():
            predictions[i] = np.dot(X[i][valid_mask], w[valid_mask]) + b
        else:
            predictions[i] = b  # å¦‚æœæ‰€æœ‰ feature éƒ½æ˜¯ NaNï¼Œåªç”¨ bias
    
    return predictions


def save_model(model_path, feature_names, w, b):
    model_data = {
        "feature_names": feature_names,
        "weights": w.tolist(),
        "bias": float(b)
    }
    with open(model_path, 'w', encoding='utf-8') as f:
        json.dump(model_data, f, ensure_ascii=False, indent=4)
    print(f"âœ… Model saved to {model_path}")


def load_model(model_path):
    if not os.path.exists(model_path):
        return None, None, None
    with open(model_path, 'r', encoding='utf-8') as f:
        model_data = json.load(f)
    feature_names = model_data["feature_names"]
    w = np.array(model_data["weights"], dtype=float)
    b = float(model_data["bias"])
    print(f"âœ… Model loaded from {model_path}")
    return feature_names, w, b


# ========== Data Preparation Helper ==========
def extract_features_and_target(train_df, feature_items, target_item, value_columns):
    """
    å°‡ train_df ä¸­ç‰¹å®š ItemName çš„æ•¸æ“šå–å‡ºï¼Œçµ„æˆ X (features) èˆ‡ y (target)
    å…è¨± feature ç‚º NaN
    """
    # å»ºç«‹ä¸€å€‹ dictï¼Œkey æ˜¯ item nameï¼Œvalue æ˜¯æ¯å°æ™‚çš„å€¼
    item_data = {}
    for item in feature_items + [target_item]:
        subset = train_df[train_df["ItemName"].str.strip() == item]
        if subset.empty:
            # å¦‚æœè©² feature ä¸å­˜åœ¨ï¼Œç”¨å…¨ NaN å¡«å……
            print(f"âš ï¸ Feature '{item}' not found, using NaN values")
            item_data[item] = np.full(len(value_columns), np.nan)
        else:
            item_data[item] = subset.iloc[0, value_columns].to_numpy(dtype=float)

    # çµ„æˆ X å’Œ y
    X = np.stack([item_data[f] for f in feature_items], axis=1)  # shape (24, n_features)
    y = item_data[target_item]                                   # shape (24,)

    return X, y

# ========== Testing Function ==========

def predict_next_hour_pm25(test_path, model_path="./model.json", cur_hour=9):
    next_hour = cur_hour + 1
    """
    ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹é æ¸¬æ¯ç­†è³‡æ–™çš„ç¬¬next_hourå°æ™‚ PM2.5ã€‚
    æ”¯æ´ feature ç‚º NaN çš„æƒ…æ³ï¼Œä¸¦é æ¸¬æ‰€æœ‰æ—¥æœŸçš„ PM2.5
    """
    # === è®€å– test.csvï¼Œä¸¦æ‰‹å‹•å‘½åæ¬„ä½ ===
    columns = ["Date", "ItemName"] + [str(i) for i in range(1, 11)]
    test_df = pd.read_csv(test_path, header=None, names=columns)
    
    test_df = data_cleaning(test_df, value_columns=[i+2 for i in range(cur_hour)], invalid_pattern='#|A|X|\*')
    test_df = data_interpolation(test_df, value_columns=[i+2 for i in range(cur_hour)])
    # ä¸åˆªé™¤ä»»ä½•æ•¸æ“šï¼Œä¿ç•™æ‰€æœ‰æ—¥æœŸ
    test_df = data_dropdown(test_df, value_columns=[i+2 for i in range(cur_hour)], drop_type='date', skip_for_test=True)
    print("âœ… Test data loaded and cleaned.")

    # === è¼‰å…¥æ¨¡å‹ ===
    feature_names, w, b = load_model(model_path)
    if w is None or feature_names is None:
        raise ValueError("âŒ Model not found or feature mismatch with trained model.")

    value_columns = [i+2 for i in range(cur_hour)]  # ç¬¬1~cur_hourå°æ™‚æ¬„ä½ (index=2~cur_hour+1)
    results = []
    y_true_list = []
    y_pred_list = []

    # å–å¾—æ‰€æœ‰å”¯ä¸€çš„æ—¥æœŸ
    all_dates = test_df["Date"].unique()
    print(f"ğŸ“Š Total dates to predict: {len(all_dates)}")

    for date in all_dates:
        subset = test_df[test_df["Date"] == date]

        # æ”¶é›†è©²æ™‚æ®µçš„ feature è³‡æ–™ï¼ˆå…è¨± NaNï¼‰
        feature_data = {}
        try:    
            for item in feature_names:
                row = subset[subset["ItemName"].str.strip() == item]
                if row.empty:
                    # å¦‚æœè©² feature ä¸å­˜åœ¨ï¼Œä½¿ç”¨ NaN
                    feature_data[item] = np.full(len(value_columns), np.nan)
                else:
                    feature_data[item] = row.iloc[0, value_columns].to_numpy(dtype=float)
        except Exception as e:
            print(f"âš ï¸ Error processing date {date}: {e}")
            continue

        # å–ç¬¬cur_hourå°æ™‚ feature å€¼ä½œç‚ºè¼¸å…¥ï¼ˆå…è¨± NaNï¼‰
        X_input = np.array([feature_data[f][-1] for f in feature_names]).reshape(1, -1)

        # é æ¸¬ç¬¬next_hourå°æ™‚ PM2.5
        y_pred_next_hour = predict(X_input, w, b)[0]
        results.append({"index": date, "answer": y_pred_next_hour})
        y_pred_list.append(y_pred_next_hour)

        # è‹¥æœ‰ PM2.5 çœŸå¯¦ç¬¬next_hourå°æ™‚æ•¸æ“šï¼Œå‰‡è¨ˆç®— RMSE
        pm25_row = subset[subset["ItemName"].str.strip() == "PM2.5"]
        if not pm25_row.empty and str(next_hour) in pm25_row.columns:
            true_val = pm25_row.iloc[0][str(next_hour)]
            if not pd.isna(true_val):
                y_true_list.append(float(true_val))

    result_df = pd.DataFrame(results)
    print(f"âœ… Prediction complete. Predicted {len(result_df)} dates.")

    # === RMSE è¨ˆç®— ===
    if len(y_true_list) == len(y_pred_list) and len(y_true_list) > 0:
        rmse = np.sqrt(np.mean((np.array(y_true_list) - np.array(y_pred_list)) ** 2))
        print(f"âœ… Prediction RMSE = {rmse:.4f}")
    else:
        rmse = None
        print("âš ï¸ RMSE cannot be calculated (missing true PM2.5 values).")

    return result_df, rmse

def unique_filename(file_path):
    base, ext = os.path.splitext(file_path)
    counter = 1
    new_file_path = file_path
    while os.path.exists(new_file_path):
        new_file_path = f"{base}_v{counter}{ext}"
        counter += 1
    return new_file_path

def main_training(important_features=None, name=None, iterations=1000000, model_path=None):
    train_df = read_csv("./data/train.csv")
    
    # å°æ¯å€‹ row ä¸­çš„ value_columnsï¼Œ
    # å¦‚æœæ˜¯ç©ºå€¼æˆ–åŒ…å« invalid çš„è³‡æ–™ (ex: #, A, X, *) ï¼Œå°±æ”¹ç‚º NaN
    value_columns = [i+3 for i in range(0, 24)]
    invalid_values = ['#', 'A', 'X', '*']
    invalid_pattern = '|'.join(map(re.escape, invalid_values))
    
    print("âœ… Data loaded.")

    # å°‡ train_df åšè³‡æ–™æ¸…ç†
    train_df = data_cleaning(train_df, value_columns, invalid_pattern)
    train_df = data_interpolation(train_df, value_columns)
    train_df = data_dropdown(train_df, value_columns, drop_type='date')

    print(f"âœ… Data cleaned. Remaining rows: {len(train_df)}")

    # --- é¸æ“‡ Feature èˆ‡ Target ---
    if important_features is None:
        important_features = [
            'AMB_TEMP',
            'CH4',
            'CO',
            'NMHC',
            'NO',
            'NO2',
            'NOx',
            'O3',
            'PM10',
            'PM2.5',
            'RAINFALL',
            'RH',
            'SO2',
            'THC',
            'WD_HR',
            'WIND_DIREC',
            'WIND_SPEED',
            'WS_HR'
        ]
    
    feature_items = important_features

    if name is not None:
        output_dir = f"./history_model/{name}"
    else:
        output_dir = "./history_model" + f"/{'_'.join(feature_items)}"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    target_item = "PM2.5"

    # --- å»ºç«‹è³‡æ–™ ---
    X, y = extract_features_and_target(train_df, feature_items, target_item, value_columns)
    print(f"âœ… X shape: {X.shape}, y shape: {y.shape}")
    print(f"NaN count in X: {np.isnan(X).sum()} out of {X.size} values ({np.isnan(X).sum()/X.size*100:.2f}%)")
    print(f"NaN count in y: {np.isnan(y).sum()} out of {y.size} values")

    # --- æ¨¡å‹è·¯å¾‘ ---
    if model_path is None:
        model_path = "./model.json"

    # --- è¼‰å…¥ç¾æœ‰æ¨¡å‹ï¼ˆè‹¥æœ‰ï¼‰ ---
    feature_names, w, b = load_model(model_path)

    # --- è‹¥ feature ä¸åŒæˆ–æ¨¡å‹ä¸å­˜åœ¨ï¼Œé‡æ–°åˆå§‹åŒ– ---
    if w is None or feature_names != feature_items:
        print("ğŸ” No existing model found or feature mismatch. Starting new training.")
        w, b = None, None
    
    # --- è¨“ç·´æ¨¡å‹ï¼ˆå¯çºŒè¨“ï¼‰ ---
    w, b = train_linear_regression(X, y, w=w, b=b, lr=0.0001, iterations=iterations, verbose=True)

    # --- å„²å­˜æ¨¡å‹ ---
    save_model(model_path, feature_items, w, b)
    save_model(unique_filename(f"{output_dir}/model.json"), feature_items, w, b)

    # --- é©—è­‰ ---
    y_pred = predict(X, w, b)
    rmse = np.sqrt(np.mean((y - y_pred) ** 2))
    print(f"âœ… Final RMSE = {rmse:.4f}")

import argparse

if __name__ == "__main__":
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', type=str, default=None, help='Path to the model file (JSON)')

    args = parser.parse_args()
    model_path = args.model if args.model is not None else "./model.json"

    important_features = [
        'SO2',
        'CH4',
        'CO',
        'AMB_TEMP',
        'RAINFALL',
        'WIND_SPEED'
    ]
    
    # important_features = [
    #     "PM2.5",
    #     "PM10",
    #     "NO2",
    #     "NOx",
    #     "SO2",
    #     "CH4",
    #     "CO",
    #     "RH",
    #     "AMB_TEMP",
    #     "RAINFALL",
    #     "WIND_SPEED"
    # ]
    
    for i in range(1):    
        # Train the model
        # main_training(important_features=important_features, name=None, iterations=10000, model_path=model_path)

        # --- predict test data ---
        test_path = "./data/test.csv"
        
        # ä½¿ç”¨å‰ 9 å°æ™‚çš„è³‡æ–™ä¾†é æ¸¬ç¬¬ 10 å°æ™‚çš„ PM2.5
        cur_hour = 9
        pred_df, rmse = predict_next_hour_pm25(
            test_path, 
            model_path=model_path, 
            cur_hour=cur_hour
        )
        
        clean_model_path = os.path.splitext(os.path.basename(model_path))[0]
        clean_model_path = f"new_10000_v{i}"

        if rmse is not None:
            rmse = f"{rmse:.4f}"
    
        print(pred_df.head())
        pred_df.to_csv(f"./prediction_{cur_hour + 1}_hour_pm25_{clean_model_path}_rmse_{rmse}.csv", index=False)
        print(f"âœ… Saved prediction to prediction_{cur_hour + 1}_hour_pm25_{clean_model_path}_rmse_{rmse}.csv")