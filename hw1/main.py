import re, os, json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def read_csv(file_path):
    """Reads a CSV file and returns a pandas DataFrame."""
    return pd.read_csv(file_path)

def data_cleaning(data_df, value_columns, invalid_pattern):
    # å°æ¯å€‹æ¬„ä½é€²è¡Œæª¢æŸ¥èˆ‡è½‰æ›
    for col in value_columns:
        # æŠŠåŒ…å« invalid çš„åœ°æ–¹è¨­ç‚º NaN
        data_df.iloc[:, col] = data_df.iloc[:, col].astype(str)  # ç¢ºä¿æ˜¯å­—ä¸²
        data_df.iloc[:, col] = data_df.iloc[:, col].mask(
            data_df.iloc[:, col].str.contains(invalid_pattern, case=False, na=False),
            np.nan
        )
    
    return data_df

# è³‡æ–™å…§æ’
def data_interpolation(data_df, value_columns):
    for i, row in data_df.iterrows():
        # æ‰¾å‡ºè©² row ä¸­ä¸æ˜¯ NaN çš„ value_columns çš„ index
        not_nan = row.iloc[value_columns].notna().to_numpy().nonzero()[0]
        # å¦‚æœæƒ³ä¸åš interpolation çš„è©±ï¼Œéœ€è¦æŠŠé€™äº› row è·³é
        if len(not_nan) == len(value_columns) or len(not_nan) == 0:
            continue
        
        # å¦‚æœè©² row ä¸­æœ‰å¤šå€‹ value_columns ä¸æ˜¯ NaNï¼Œå°±ç”¨ç·šæ€§æ’å€¼æ³• (linear interpolation) è£œå€¼
        np_row = row.iloc[value_columns].to_numpy(dtype=float)
        x = np.arange(len(np_row))
        data_df.iloc[i, value_columns] = np.interp(x, x[not_nan], np_row[not_nan])

        # if len(not_nan) == 1:
        #     print(f"Row {i} \t, {row.Date}\t, {row.ItemName.strip()}\t has only 1 value, set interpolation:")
        #     print(data_df.iloc[i, value_columns].to_numpy(dtype=float))
    
    return data_df

def data_dropdown(data_df, value_columns, drop_type='item'):
    dropdown_items = []
    for i, row in data_df.iterrows():
        not_nan = row.iloc[value_columns].notna().to_numpy().nonzero()[0]

        # å¦‚æœ row ä¸­çš„ value_columns çš„é NaN < thresholdï¼Œå°±æŠŠè©² row åˆªæ‰
        if len(not_nan) < 2:
            dropdown_items.append({'date': row.Date, 'feature': row.ItemName})

    # for item in dropdown_items:
    #     print(f"Drop Date: {item['date']}, Feature: {item['feature']}")

    for item in dropdown_items:
        if drop_type == 'item':
            # åˆªæ‰ç¬¦åˆè©² date ä¸” feature çš„ row
            data_df = data_df[~((data_df.Date == item['date']) & (data_df.ItemName == item['feature']))]
        elif drop_type == 'date':
            data_df = data_df[data_df.Date != item['date']]
        elif drop_type == 'feature':
            data_df = data_df[data_df.ItemName != item['feature']]
        
    return data_df


# ========== Linear Regression Function ==========

def train_linear_regression(X, y, w=None, b=None, lr=0.01, iterations=1000, verbose=True):
    """
    ä½¿ç”¨ Gradient Descent è¨“ç·´ Linear Regression æ¨¡å‹
    æ”¯æ´ç¹¼çºŒè¨“ç·´ï¼ˆè‹¥ w, b å·²å­˜åœ¨ï¼‰
    """
    n_samples, n_features = X.shape

    # åˆå§‹åŒ–æ¬Šé‡
    if w is None:
        w = np.zeros(n_features)
    if b is None:
        b = 0.0

    for i in range(iterations):
        y_pred = X.dot(w) + b
        error = y_pred - y

        dw = (1 / n_samples) * X.T.dot(error)
        db = (1 / n_samples) * np.sum(error)

        w -= lr * dw
        b -= lr * db

        if verbose and i % 100000 == 0:
            rmse = np.sqrt(np.mean(error ** 2))
            print(f"Iteration {i:4d} | RMSE = {rmse:.4f}")

    return w, b


def predict(X, w, b):
    return X.dot(w) + b


def save_model(model_path, feature_names, w, b):
    model_data = {
        "feature_names": feature_names,
        "weights": w.tolist(),
        "bias": float(b)
    }
    with open(model_path, 'w', encoding='utf-8') as f:
        json.dump(model_data, f, ensure_ascii=False, indent=4)
    print(f"âœ… Model saved to {model_path}")


def load_model(model_path):
    if not os.path.exists(model_path):
        return None, None, None
    with open(model_path, 'r', encoding='utf-8') as f:
        model_data = json.load(f)
    feature_names = model_data["feature_names"]
    w = np.array(model_data["weights"], dtype=float)
    b = float(model_data["bias"])
    print(f"âœ… Model loaded from {model_path}")
    return feature_names, w, b


# ========== Data Preparation Helper ==========
def extract_features_and_target(train_df, feature_items, target_item, value_columns):
    """
    å°‡ train_df ä¸­ç‰¹å®š ItemName çš„æ•¸æ“šå–å‡ºï¼Œçµ„æˆ X (features) èˆ‡ y (target)
    """
    # å»ºç«‹ä¸€å€‹ dictï¼Œkey æ˜¯ item nameï¼Œvalue æ˜¯æ¯å°æ™‚çš„å€¼
    item_data = {}
    for item in feature_items + [target_item]:
        subset = train_df[train_df["ItemName"].str.strip() == item]
        if subset.empty:
            continue
        item_data[item] = subset.iloc[0, value_columns].to_numpy(dtype=float)

    # æª¢æŸ¥æ˜¯å¦æ‰€æœ‰ feature éƒ½æœ‰è³‡æ–™
    if any(item not in item_data for item in feature_items + [target_item]):
        missing = [i for i in feature_items + [target_item] if i not in item_data]
        raise ValueError(f"âŒ Missing items: {missing}")

    # çµ„æˆ X å’Œ y
    X = np.stack([item_data[f] for f in feature_items], axis=1)  # shape (24, n_features)
    y = item_data[target_item]                                   # shape (24,)

    return X, y

def unique_filename(file_path):
    base, ext = os.path.splitext(file_path)
    counter = 1
    new_file_path = file_path
    while os.path.exists(new_file_path):
        new_file_path = f"{base}_v{counter}{ext}"
        counter += 1
    return new_file_path

def main(important_features=None, name=None, iterations=1000000):
    train_df = read_csv("./data/train.csv")
    test_df = read_csv("./data/test.csv")

    # å°æ¯å€‹ row ä¸­çš„ value_columnsï¼Œ
    # å¦‚æœæ˜¯ç©ºå€¼æˆ–åŒ…å« invalid çš„è³‡æ–™ (ex: #, A, X, *) ï¼Œå°±æ”¹ç‚º NaN
    value_columns = [i+3 for i in range(0, 24)]
    invalid_values = ['#', 'A', 'X', '*']
    invalid_pattern = '|'.join(map(re.escape, invalid_values))  # â†’ "#|A|X|\*"
    
    print("âœ… Data loaded.")

    # å°‡ train_df åšè³‡æ–™æ¸…ç†ï¼Œæœ‰ invalid pattern çš„åœ°æ–¹è¨­ç‚º NaN
    # å…¨éƒ¨æ˜¯ NaN çš„ row å°±åˆªæ‰ï¼Œå°‘é‡çš„ NaN ç”¨ç·šæ€§æ’å€¼æ³•è£œå€¼
    train_df = data_cleaning(train_df, value_columns, invalid_pattern)
    train_df = data_interpolation(train_df, value_columns)
    train_df = data_dropdown(train_df, value_columns, drop_type='date')

    print(f"âœ… Data cleaned. Remaining rows: {len(train_df)}")

    # --- é¸æ“‡ Feature èˆ‡ Target ---
    feature_items = [
        'AMB_TEMP',
        'CH4',
        'CO',
        'NMHC',
        'NO',
        'NO2',
        'NOx',
        'O3',
        'PM10',
        'PM2.5',
        'RAINFALL',
        'RH',
        'SO2',
        'THC',
        'WD_HR',
        'WIND_DIREC',
        'WIND_SPEED',
        'WS_HR'
    ]
    
    feature_items = important_features  # ä½¿ç”¨è¼ƒå°‘çš„ feature ä¾†è¨“ç·´

    if name is not None:
        output_dir = f"./history_model/{name}_" + f"{'_'.join(feature_items)}"
    else:
        output_dir = "./history_model" + f"/{'_'.join(feature_items)}"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    target_item = "PM2.5"

    # --- å»ºç«‹è³‡æ–™ ---
    X, y = extract_features_and_target(train_df, feature_items, target_item, value_columns)
    print(f"âœ… X shape: {X.shape}, y shape: {y.shape}")
    print(f"check NaN in X and y: {np.isnan(X).any()}, {np.isnan(y).any()}")
    print(f"check Inf in X and y: {np.isinf(X).any()}, {np.isinf(y).any()}")

    # --- æ¨¡å‹è·¯å¾‘ ---
    model_path = "./model.json"

    # --- è¼‰å…¥ç¾æœ‰æ¨¡å‹ï¼ˆè‹¥æœ‰ï¼‰ ---
    feature_names, w, b = load_model(model_path)

    # --- è‹¥ feature ä¸åŒæˆ–æ¨¡å‹ä¸å­˜åœ¨ï¼Œé‡æ–°åˆå§‹åŒ– ---
    if w is None or feature_names != feature_items:
        print("ğŸ” No existing model found or feature mismatch. Starting new training.")
        w, b = None, None
    
    # # --- è¨“ç·´æ¨¡å‹ï¼ˆå¯çºŒè¨“ï¼‰ ---
    # w, b = train_linear_regression(X, y, w=w, b=b, lr=0.0001, iterations=iterations, verbose=True)

    # # --- å„²å­˜æ¨¡å‹ ---
    # save_model(model_path, feature_items, w, b)
    # save_model(unique_filename(f"{output_dir}/model.json"), feature_items, w, b)  # å‚™ä»½
    print("âœ… Model training skipped.")

    # --- é©—è­‰ ---
    y_pred = predict(X, w, b)
    rmse = np.sqrt(np.mean((y - y_pred) ** 2))
    print(f"âœ… Final RMSE = {rmse:.4f}")

if __name__ == "__main__":
    
    # important_features = [
    #     'PM2.5',
    #     'PM10',
    #     'NO2',
    #     'NOx',
    #     'SO2',
    #     'CH4',
    #     'CO',
    #     'RH',
    #     'AMB_TEMP',
    #     'RAINFALL',
    #     'WIND_SPEED'
    # ]
    
    # for _ in range(20):
    #     main(important_features=important_features, name="all", iterations=100000)
    
    # for _ in range(20):
    #     main(important_features=important_features, name="all_1000000", iterations=100000)
    
    # os.remove("./model.json")  # åˆªæ‰æ¨¡å‹ï¼Œé¿å…å½±éŸ¿ä¸‹ä¸€æ¬¡çš„è¨“ç·´
    
    important_features = [
        # 'PM2.5',
        # 'PM10',
        # 'NO2',
        # 'NOx',
        'SO2',
        'CH4',
        'CO',
        # 'RH',
        'AMB_TEMP',
        'RAINFALL',
        'WIND_SPEED'
    ]
    
    main(important_features=important_features, name="imp_10^7", iterations=10000000)